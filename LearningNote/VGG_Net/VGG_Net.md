# VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION

[toc]

- 词短句翻译

  investigate：调查，研究

  thorough：彻底的，完全的

  prior-art：先前技术

  localization：本地化，定位

  secure：获得

  generalize：推广，泛化

  facilitate：促进

  cluster：簇，集群

  utilis：利用

  to this end：为了这个目的

  assess：评估评价

  evaluate：评估评价

  revision：修订
  
  utilize：利用
  
  resolution：分辨率，解决

## ABSTRACT

卷积神经网络深度和它在大规模图像识别上的准确度之间的联系。

主要贡献：对使用了很小卷积核的网络结构，彻底清晰地分析评测了增加其深度的带来的影响，而结果显示当网络深度提升至16-19层时，先前网络能有极大的提升。这些发现是我们在Image Challenge 2014的提交上的基础，我们也分别获得了定位方面一等奖和分类方面二等奖（These findings were the basis of our ImageNet Challenge 2014 submission, where **our team secured the first and the second places in the localisation and classification tracks respectively**）我们也证明了我们的方法能够很好的推广到其他数据集，并实现了最先进优异的结果。我们已经公开了两个表现最好的的卷积网络模型，以促进在计算机视觉中使用深度学习的进一步研究。

## 1 INTRODUCTION

卷积神经网络在大规模的图片、影像的识别上取得了较大的成功，它的成功基于大型公共图片数据集库，比如ImageNet，以及高性能计算机系统，譬如GPU和大规模分布式集群。此外，ImageNet Large-Scale Visual Recognition Challenge在深度视觉识别体系结构的发展中发挥了重要作用，它已经成为了几代大规模图像分类系统的测试平台，从高维浅层特征编码到深层卷积网络。

随着卷积神经网络在计算机视觉领域愈加的普遍，大家做了很多关于如何提高最初的结构的准确率的尝试。举例而言（for instance），ILSVRC 2013中最佳的提交方案在网络的第一层使用了较小的接收窗口（卷积核？）和更小的步长，以及在整个图片和多个维度上对网络进行密集地训练测试。

本文中，我们指出了影响卷积网络的另一个重要因素：网络的深度。为此，我们固定了网络结构的其他参数，仅仅逐步的通过增加卷积层的方式，增加网络的深度，在所有层都是用了$3 \times 3$卷积核的条件下，这样的操作是可行的。

最后，我们提出了一个更加准确的卷积神经网络结构，不仅能在ILSVRC分类和定位任务上取得最先进的准确率，而且能够应用于其他图片识别数据集，即使该网络结构仅仅作为相对简单流程的一部分，也能够有较好的提升效果。我们已经发布了两个性能最好的模型，以供进一步的研究。

接下来的文章将以如下结构安排。

Section 2，我们会描述我们的卷积网络的配置

Section 3，图片分类网络训练和评测的细节

Section 4，在ILSVRC分类任务上对配置进行比较

Section 5，paper concludes

为了完整性，我们在附录A中描述并评估了我们的ILSVRC-2014目标定位检测系统，在附录B中则讨论了如何在其他数据集中进行深层网络特征的泛化推广，附录C主要是论文修改的列表

## 2 CONVNET CONFIGURATIONS

为在一个公平环境下验证深度增加带来的提升效果， 我们的卷积网络层用相同的原则进行设计，在这一部分我们先描述我们网络的通用层（Section. 2.1），然后详细描述评价过程中的特殊配置（Section. 2.2），我们设计选则在Section. 2.3中进行讨论同时与以往的工作进行比较。

### 2.1 ARCHITECTURE

训练过程中，我们网络的输入是固定大小的$224\times224$的RGB图片。我们唯一做的预处理就是从每个像素中减去训练集中计算得到的平均RGB的值。

图片经过一系列堆叠卷积层，卷积层中使用小视野的$3\times3$卷积核（他是捕捉左/右、上/下、中间特征的最小大小的卷积核）。在某一个配置中我们也使用了$1\times1$卷积核，可以将其视为输入通道的线性变换(在他之后再进行非线性)。

卷积的步长设置为$1$像素

卷积层对输入图片进行空间填充，使得卷积后结果与输入空间分辨率相同，对于$3\times3$卷积层，$padding = 1$

池化过程由五个最大池化层进行，部分卷积层后会有池化层。最大池化层由$2\times2$池化窗口（卷积核，滤波器），步长为$2$

堆叠的卷积层网络（不同的结构有不同的深度）结构如下：

- 三个全连接层（is followed by three Fully-Connected (FC) layers）：

  第一层和第二层：有$4096$个通道数

  第三层：$1000$个通道用于ILSVRC的$1000$类的分类任务

- 最后一层，soft max层

所有网络的全连接层结构都相同。

所有的隐藏层都是用激励函数（ReLU），变为非线性。

我们需要说明我们的网络结构，除一个外，都没有包含局部性应归一化。正如Section 4展示的结果那样， 归一化对网络在ILSVRC数据集上的表现并没有提升效果，只会导致内存空间的损耗和计算时间的耗费。关于使用LRN层的参数设置如（Krizhevsky et al., 2012）

### 2.2 CONFIGURATIONS

卷积层配置如下：

![image-20201215114043218](assets/image-20201215114043218.png)

接下来，我们会使用图中的$A-E$称呼各个网络。所有网络的配置都遵循Section. 2.1中介绍的通用设计，唯有深度，每个网络不同，从$A$的$11$层权值层（$8$个卷积层和$3$个全连接层）到$E$中的$19$个权值层（$16$个卷积层和$3$个全连接层）。卷积层的宽度（卷积层的通道数很小）很小，从第一层的$64$个通道经过每一次最大池化后变大两倍最后到达$512$。

下表中，我们展示了每个配置的参数量

![image-20201215131741044](assets/image-20201215131741044.png)

虽然网的深度很大，但我们的深度网中权值（参数）的数量并不比卷积层更大感受野更大的的浅层网中权值的数量更大。

### 2.3 DISCUSSION

我们的卷积神经网络与ILSVRC-20212和ILSVRC-2013比赛中表现最好的网络结构上有很大的差异。相比于在第一卷积层中使用相对比较大的感受野（卷积核$11\times11$，步长为$4$，或是卷积核$7\times7$，步长为$2$），我们整体网络都使用了较小的$3\times3$感受野（卷积核），步长为$1$实现了与输入进行每一个像素的卷积。

可以看出两层$3\times3$卷积层的堆叠（不含空间池化过程）能实现相当于$5\times5$的感受野，三层$3\times3$卷积可以拥有$7\times7$的感受野。那么$3\times3$卷积的堆叠和单一的$7\times7$卷积的区别在哪里呢？

- 用三个非线性修正层代替单一非线性修正层，能够让决策函数有更强的鉴别能力

- 减少了参数数量

  假设输入和三层$3\times3$卷积层堆叠的输出都为$C$个通道数时，堆叠方式的参数量为$3(3^2C^2)=27C^2$，而单一$7\times7$的卷积层则有$7^2C^2=49C^2$个参数，将近减少81
  %左右的数量。

  这可以看作是对$7\times7$普通滤波器进行正则化，迫使它们通过$3×3$滤波器进行分解(在两者之间注入非线性)。

而$C$网络中的$1×1$卷积层则是一种增加决策函数非线性特性而不影响卷积层感受野的方法。